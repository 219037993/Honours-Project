# -*- coding: utf-8 -*-
"""HonoursProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mKH9x0JSzcvnkpRsxoouGZ-CLscI8YFj

#A Comparison of Automatic Music Genre Classification Deep Learning Techniques
#Christian Francis
#219037993

#Imports

Mounting Drive
"""

#from google.colab import drive
 #drive.mount('/content/drive')

"""Importing Libraries"""

import pandas as pd
import time
import numpy as np 
import matplotlib.pyplot as plt
import scipy
import os
import pickle
import librosa
import librosa.display
from IPython.display import Audio
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.model_selection import cross_val_score, StratifiedKFold

#data = pd.read_csv("/content/drive/MyDrive/Honours/Data/features_3_sec.csv")
data = pd.read_csv("/content/features_3_sec.csv")

"""#Visualization"""

#Data preprocessing and visualization were inspired by the following notebook:

"""
Title: Music Genre Classification using CNN
Author:Sharma,S
Date: 2021
Code Version: 3.0
Availability:https://www.kaggle.com/code/soumyasharma20/music-genre-classification-using-cnn/notebook#Feature-Extraction

"""

data=data.drop(labels="filename",axis=1)

audio_recording="/content/country.00050.wav"
d,sr=librosa.load(audio_recording)
print(type(data),type(sr))

"""Raw wave files"""

plt.figure(figsize=(12,4))
librosa.display.waveplot(d,color="#2B4F72")
plt.show()

"""Spectrogram"""

stft=librosa.stft(d)
stft_db=librosa.amplitude_to_db(abs(stft))
plt.figure(figsize=(14,6))
librosa.display.specshow(stft,sr=sr,x_axis='time',y_axis='hz')
plt.colorbar()

stft=librosa.stft(d)
stft_db=librosa.amplitude_to_db(abs(stft))
plt.figure(figsize=(14,6))
librosa.display.specshow(stft_db,sr=sr,x_axis='time',y_axis='hz')
plt.colorbar()

"""Spectral Roll off"""

spectral_rolloff=librosa.feature.spectral_rolloff(d+0.01,sr=sr)[0]
plt.figure(figsize=(14,6))
librosa.display.waveplot(d,sr=sr,alpha=0.4,color="#2B4F72")

"""Chroma Feature"""

import librosa.display as lplt
chroma = librosa.feature.chroma_stft(d,sr=sr)
plt.figure(figsize=(14,6))
lplt.specshow(chroma,sr=sr,x_axis="time",y_axis="chroma",cmap="coolwarm")
plt.colorbar()
plt.title("Chroma Features")
plt.show()

"""Zero Crossing Rate"""

start=1000
end=1200
plt.figure(figsize=(12,4))
plt.plot(d[start:end],color="#2B4F72")
plt.grid()

zero_cross_rate=librosa.zero_crossings(d[start:end],pad=False)
print("zero_crossings :", sum(zero_cross_rate))

"""#Data Preprocessing

#Feature Extraction
"""

# Convert categorical data into numerical data that the model will understand
classList=data.iloc[:,-1]
labelEnc= LabelEncoder()

# Fitting label encoder and return the encoded labels
y=labelEnc.fit_transform(classList)

print(data.iloc[:,:-1])

"""#Scaling Features"""

#Standard Scaler from the sklearn library is used to standardize the features by removing mean and scaling to unit variance
fit=StandardScaler()

X= fit.fit_transform(np.array(data.iloc[:, : -1], dtype=float))

"""#Splitting Training and Testing Sets"""

#I plan to test with training sets of 90%, 75%, 66%

#90% Training, 10% testing
#Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.10) 

#70% Training, 25% testing
#Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.25) 

#66% Training, 34% testing
Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.34)

#print( "length of ytest :", len(ytest))
#print("Length of ytrain",len(ytrain))

"""#Model

#CNN
"""

def cnn_model():

    model=tf.keras.models.Sequential([
    tf.keras.layers.Dense(512,activation='relu',input_shape=(Xtrain.shape[1],)),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(256,activation='relu'),
    keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(10,activation='softmax'),

])
    
    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')
    return model

model = cnn_model()
model.summary()

#Cross validation implementation on CNN and RNN inspired by the folloiwng notebook:

"""
Title: Basic CNN Keras with cross validation
Author:Muchahary, F
Date: 2018
Code Version: 4.0
Availability:https://www.kaggle.com/code/franklemuchahary/basic-cnn-keras-with-cross-validation

"""

#set early stopping criteria

pat = 5 #this is the number of epochs with no improvment after which the training will stop
early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)

#define the model checkpoint callback -> this will keep on saving the model as a physical file
model_checkpoint = ModelCheckpoint('cnn_check.h5', verbose=1, save_best_only=True)

def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=600, BATCH_SIZE=128):
    model = None
    model = cnn_model()
    results = model.fit(t_x, t_y,validation_data=(val_x,val_y),epochs=EPOCHS,batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint])  
    print("Val Score: ", model.evaluate(val_x, val_y))
    return results

n_folds=10
epochs=20
batch_size=128

#save the model history in a list after fitting so that we can plot later
model_history = [] 

for i in range(n_folds):
    print("Training on Fold: ",i+1)

    #need to change this value as well when testing

    t_x, val_x, t_y, val_y = train_test_split(Xtrain, ytrain, test_size=0.34, 
                                               random_state = np.random.randint(1,1000, 1)[0])
    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))

    print("======="*12, end="\n\n\n")

"""Cnn Model Evaluation

"""

#Load the model that was saved by ModelCheckpoint
model = load_model('cnn_check.h5')

loss,accuracy=model.evaluate(Xtest,ytest,batch_size=128)
print("Test Loss :",loss)
print("Test Accuracy :",accuracy)

"""#RNN"""

def rnn_model():

    model=tf.keras.models.Sequential([
    tf.keras.layers.LSTM(512,input_shape=(Xtrain.shape[1],1),return_sequences= True),
    tf.keras.layers.LSTM(512),
        # dense layer
    tf.keras.layers.Dense(256,activation='relu'),
    keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dropout(0.2),


    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.Dropout(0.2),  

    
     # output layer
    tf.keras.layers.Dense(10,activation='softmax'),

])
    
    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')
    return model


model = rnn_model()
model.summary()

#set early stopping criteria

pat = 5 #this is the number of epochs with no improvment after which the training will stop
early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)

#define the model checkpoint callback -> this will keep on saving the model as a physical file
model_checkpoint = ModelCheckpoint('rnn_check.h5', verbose=1, save_best_only=True)

def rnn_fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=600, BATCH_SIZE=128):
    model = None
    model = rnn_model()
    results = model.fit(t_x, t_y,validation_data=(val_x,val_y),epochs=EPOCHS,batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint])  
    print("Val Score: ", model.evaluate(val_x, val_y))
    return results

n_folds=10
epochs=20
batch_size=128

#save the model history in a list after fitting so that we can plot later
model_history = [] 

for i in range(n_folds):
    print("Training on Fold: ",i+1)

    #need to change this value as well when testing

    t_x, val_x, t_y, val_y = train_test_split(Xtrain, ytrain, test_size=0.34, 
                                               random_state = np.random.randint(1,1000, 1)[0])
    model_history.append(rnn_fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))
    
    print("======="*12, end="\n\n\n")

"""RNN Model Evaluation"""

#Load the model that was saved by ModelCheckpoint
model = load_model('rnn_check.h5')

loss,accuracy=model.evaluate(Xtest,ytest,batch_size=128)
print("Test Loss :",loss)
print("Test Accuracy :",accuracy)

"""#Multilayer Perceptron"""

kfold = StratifiedKFold(n_splits=10)
clf=MLPClassifier(alpha=0.01, 
                   batch_size=256, 
                   epsilon=1e-08, 
                   hidden_layer_sizes=(300,), 
                   learning_rate='adaptive', 
                   max_iter=400)
#clf.fit(Xtrain,ytrain)
#y_pred=clf.predict(Xtest)

cv_result= cross_val_score(clf, Xtrain, y = ytrain, scoring = "accuracy", cv = kfold, n_jobs=4)
cv_means=cv_result.mean()
cv_std=cv_result.std()

"""MLP Classifier Evaluation"""

print( "mean accuracy:", cv_means)